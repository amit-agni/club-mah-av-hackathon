---
title: "Initial"
author: "Amit Agni"
date: "03/05/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


if (!require("pacman")) install.packages("pacman")
p_load(caret,data.table,dplyr,lubridate
       ,here,DescTools,DataExplorer,tidyverse
       ,googledrive,e1071,ggcorrplot,forcats
       ,xgboost,DiagrammeR,ROCR,cutpointr
       ,MlBayesOpt,rBayesianOptimization)

options(scipen=999) #remove scientific notation in printing 


####### parallel on windows #######
p_load(doParallel)
cl <- makeCluster(detectCores(), type='PSOCK')
#registerDoParallel(1)
registerDoParallel(cl)

# turn parallel processing off and run sequentially again:
# registerDoSEQ()

#####################################
# parallel::detectCores()
# 
# p_load(doMC)
# 
# registerDoMC(cores = 4)
# 
# dribble <- drive_find(pattern="train.csv")
# drive_download(as_dribble(dribble),overwrite = TRUE)
# 
# dribble <- drive_find(pattern="test.csv")
# drive_download(as_dribble(dribble),overwrite = TRUE)
# 
# DT_train <- fread(here::here("train.csv"))
# DT_test <- fread(here::here("test.csv"))
#####################################


DT_train <- fread(here::here("100_data_raw-input","train.csv"))
DT_test <- fread(here::here("100_data_raw-input","test.csv"))
DT_train$flag <- "train"
DT_test$flag <- "test"
DT_test$amount_spent_per_room_night_scaled <- 0

DT <- rbind(DT_train,DT_test)
#rm(DT_train)
#rm(DT_test)

#Date fields
vars <- c("booking_date","checkin_date","checkout_date")
DT[,(vars) := lapply(.SD,dmy), .SDcols = vars]

#Convert to factors
vars <- c("channel_code","main_product_code","persontravellingid",
          "resort_region_code","resort_type_code","room_type_booked_code",
          "state_code_resort","booking_type_code")
DT[,(vars) := lapply(.SD,as.factor), .SDcols = vars]

str(DT)

```


### Feat Eng

```{r}

#Fixing outliers
DT[, `:=`(booking_date = if_else(booking_date > checkin_date ,checkin_date , booking_date),
          booking_date_outlier = as.factor(if_else(booking_date > checkin_date ,1 , 0)))]

DT[, `:=`(roomnights = ifelse(roomnights < 0, 1,roomnights),
          roomnights_outlier = as.factor(ifelse(roomnights < 0, 1,0)))]

DT[, total_pax_outlier1 := as.factor(case_when(
          total_pax == numberofadults + numberofchildren ~ 0,
          total_pax > numberofadults + numberofchildren ~ 1,
          total_pax < numberofadults + numberofchildren ~ -1,
          TRUE ~ 777))]

DT[, total_pax_outlier2 := as.factor(case_when(
          total_pax == numberofadults  ~ 0,
          total_pax > numberofadults  ~ 1,
          total_pax < numberofadults  ~ -1,
          TRUE ~ 777))]


# prop.table(table(DT$booking_date_outlier))
# prop.table(table(DT$roomnights_outlier))
# prop.table(table(DT$total_pax_outlier))


# Fixing missing values
DT[which(complete.cases(DT)== FALSE)]
vars <- c("season_holidayed_code","state_code_residence")

prop.table(table(DT$state_code_residence))
DT[, `:=`(state_code_residence = ifelse(is.na(state_code_residence),8,state_code_residence),
          state_code_residence_outlier = ifelse(is.na(state_code_residence),1,0))]

prop.table(table(DT$season_holidayed_code))
DT[, `:=`(season_holidayed_code = ifelse(is.na(season_holidayed_code),2,season_holidayed_code),
          season_holidayed_code_outlier = ifelse(is.na(season_holidayed_code),1,0))]

DT[which(complete.cases(DT)== FALSE)]




#Date features

vars <- c("booking_date","checkin_date")
vars_temp <- paste0(vars,"_year")
DT[,(vars_temp) := lapply(.SD, function(x) as.factor(year(x)))
   , .SDcols = vars]

vars_temp <- paste0(vars,"_month")
DT[,(vars_temp) := lapply(.SD, function(x) as.factor(month(x)))
   , .SDcols = vars]

vars_temp <- paste0(vars,"_dow")
DT[,(vars_temp) := lapply(.SD, function(x) as.factor(
   if_else(wday(x,week_start = 1) %in% c(6,7), 'WE','WD')))
   , .SDcols = vars]

DT[,length_of_stay := as.double(checkout_date - checkin_date)]
DT[,lead_time := as.double(checkin_date - booking_date)]

DT[,no_of_rooms := ifelse(roomnights == 0, length_of_stay 
                           , round(roomnights/length_of_stay,0) )]

################################################
################# GROUP BY  ####################
################################################

############## Group by Number of bookings per year ############

vars <- c("main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id"
          ,"checkin_date_year","checkin_date_month","checkin_date_dow"
          ,"state_code_residence","persontravellingid"
          ,"member_age_buckets","season_holidayed_code")

for(i in vars) {
    temp <- c("flag",eval(i))
    
    DT[flag == "train",
    paste0(i,"_groupby")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_groupby")  := length(reservation_id),
    by = temp]
}




########### Total Pax outlier exploration ########
DT[,tot_pax_var1 := total_pax - (numberofadults + numberofchildren) ]

DT[,tot_pax_var2 := total_pax - numberofadults  ]

vars <- c("channel_code","booking_type_code","main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id","reservationstatusid_code"
          ,"persontravellingid"
          ,"checkin_date_year","checkin_date_month","checkin_date_dow"
          ,"member_age_buckets","season_holidayed_code")


for(i in vars) {
    temp <- c("flag",eval(i),"tot_pax_var1")
    
    DT[flag == "train",
    paste0(i,"_tot_pax_var1")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_tot_pax_var1")  := length(reservation_id),
    by = temp]
}


for(i in vars) {
    temp <- c("flag",eval(i),"tot_pax_var2")
    
    DT[flag == "train",
    paste0(i,"_tot_pax_var2")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_tot_pax_var2")  := length(reservation_id),
    by = temp]
}



############        MemberId      #############
# is this same as interactions ?
# which a random forest should be able to automatically identify ?

vars <- c("channel_code","booking_type_code","main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id"
          ,"checkin_date_year","checkin_date_month","checkin_date_dow"
          ,"state_code_residence","persontravellingid"
          ,"member_age_buckets","season_holidayed_code")

for(i in vars) {
    temp <- c("flag",eval(i),"memberid")
    
    DT[flag == "train",
    paste0(i,"_memberid")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_memberid")  := length(reservation_id),
    by = temp]
}




############    WEEKEND stay / month of stay could have mean higher spend ?      #############
# is this same as interactions ?
# which a random forest should be able to automatically identify ?

vars <- c("booking_type_code","main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id"
          ,"state_code_residence","persontravellingid"
          ,"member_age_buckets","season_holidayed_code")

for(i in vars) {
    temp <- c("flag",eval(i),"checkin_date_dow")
    
    DT[flag == "train",
    paste0(i,"_checkin_date_dow")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_checkin_date_dow")  := length(reservation_id),
    by = temp]
}


for(i in vars) {
    temp <- c("flag",eval(i),"checkin_date_month")
    
    DT[flag == "train",
    paste0(i,"_checkin_date_month")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_checkin_date_month")  := length(reservation_id),
    by = temp]
}


############## Top important feature changed to factor!!!! ############
############# Length of Stay interactions ##################

#DT$length_of_stay_new <- DT$length_of_stay

group_category(data = DT, feature = "length_of_stay", threshold = 0.001,update = TRUE)

vars <- c("booking_type_code","main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id"
          ,"checkin_date_year","checkin_date_month","checkin_date_dow"
          ,"state_code_residence","persontravellingid"
          ,"member_age_buckets","season_holidayed_code")


for(i in vars) {
    temp <- c("flag",eval(i),"length_of_stay")
    
    DT[flag == "train",
    paste0(i,"_length_of_stay")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_length_of_stay")  := length(reservation_id),
    by = temp]
}


############## Second important feature changed to factor!!!! ############
############# No of rooms interactions ##################

group_category(data = DT, feature = "no_of_rooms", threshold = 0.001, update = TRUE)

vars <- c("booking_type_code","main_product_code"
          ,"room_type_booked_code","resort_type_code","cluster_code"
          ,"resort_id"
          ,"checkin_date_year","checkin_date_month","checkin_date_dow"
          ,"state_code_residence","persontravellingid"
          ,"member_age_buckets","season_holidayed_code")

for(i in vars) {
    temp <- c("flag",eval(i),"no_of_rooms")
    
    DT[flag == "train",
    paste0(i,"_no_of_rooms")  := length(reservation_id),
    by = temp]
    
    DT[flag == "test",
    paste0(i,"_no_of_rooms")  := length(reservation_id),
    by = temp]
}






  

# Binning
group_category(data = DT, feature = "numberofadults", threshold = 0.001,update = TRUE)
group_category(data = DT, feature = "numberofchildren", threshold = 0.001,update = TRUE)
group_category(data = DT, feature = "total_pax", threshold = 0.001,update = TRUE)




#housekeeping
DT$reservation_id <- NULL
DT$memberid <- NULL

vars <- names(which(lapply(DT,is.character)==TRUE))
DT[,(vars) := lapply(.SD,as.factor), .SDcols = vars]

vars <- c("booking_date","checkin_date","checkout_date")
DT[,(vars) := NULL]

vars <- c("season_holidayed_code","state_code_residence")
DT[,(vars) := lapply(.SD,as.factor), .SDcols = vars]


vars <- grep("outlier",names(DT),value = TRUE)
DT[,(vars) := lapply(.SD,as.factor), .SDcols = vars]



############# Scale the numerical values #################
vars <- names(which(lapply(DT,is.numeric)==TRUE))
vars <- vars[!vars == "amount_spent_per_room_night_scaled"]
DT[,(vars) := lapply(.SD,scale), .SDcols = vars]




################# Polynomials ##################

# nos <- 5
# temp <- data.frame(poly(DT$ltv,nos))
# colnames(temp) <- paste0("ltv",seq(1,nos,1))
# DT <- cbind(DT,temp)
# 
# temp <- data.frame(poly(DT$asset_cost,nos))
# colnames(temp) <- paste0("asset_cost",seq(1,nos,1))
# DT <- cbind(DT,temp)





### Just to test, add the residuals of the below linear model 

# lm_model <- glm(persontravellingid  ~ 
#        resort_type_code * room_type_booked_code  ,
#     data = DT[, .(resort_type_code,room_type_booked_code,persontravellingid)],
#     famil)
# 
# summary(lm_model)
# 
# lm_model$residuals


#FEATURE SELECTION
# vars <- c("length_of_stay","no_of_rooms","total_pax","roomnights","persontravellingid","tot_bkgs_by_memberid_per_resort"
#   ,"tot_bkgs_by_memberid_per_year","resort_id","state_code_resort","lead_time","main_product_code"
#   ,"resort_region_code","resort_type_code","room_type_booked_code","numberofadults","tot_bkgs_by_memberid_per_room_type",
#   "state_code_residence","checkin_date_year",
#   "flag","amount_spent_per_room_night_scaled")
# DT <- DT[,which(names(DT) %in% vars),with = FALSE]

glimpse(DT)

```

Done in round 2
1. fixup the member ids
7. use scale/normalise - went bonkers
2. keep only top 10 features and train  - worsened



## to do
1. Need to print out the test-rmse in bayesopt
3. breakup the top 10 features to see if any other signals can be provided
4. try xgbfi
5. use vtreat - only for standaridsation etc ?
6. Eigen was used for factor analysis
8. Add interaction



length_of_stay	
no_of_rooms	
total_pax.2	
roomnights	
persontravellingid.47	
state_code_resort.4	
resort_id.9400f1b21cb527d7fa3d3eabba93557a18ebe7a2ca4e471cfe5e4c5b4ca7f767	
lead_time	
state_code_resort.6	
room_type_booked_code.2




### Model Data Prep
```{r}
DT_bkup <- copy(DT)

DT <- DT[flag == "train",]

index <- createDataPartition(y=DT$amount_spent_per_room_night_scaled, p=0.8
                             , list=FALSE) 
my_train <- DT[index,]
my_test <- DT[-index,]


my_train_labels <- my_train$amount_spent_per_room_night_scaled
my_train$amount_spent_per_room_night_scaled <- NULL

my_test_labels <- my_test$amount_spent_per_room_night_scaled
my_test$amount_spent_per_room_night_scaled <- NULL

dummies <- dummyVars(~., data = my_train, fullRank = TRUE)
my_train <- predict(dummies, newdata = my_train)

dummies <- dummyVars(~., data = my_test,fullRank = TRUE)
my_test <- predict(dummies, newdata = my_test)


my_train <- xgb.DMatrix(my_train, label = my_train_labels)
my_test <- xgb.DMatrix(my_test, label = my_test_labels)

watchlist <- list(test = my_test, train = my_train)



```




### Bayesian Optimisation (PART I)

```{r}
opt_fn <- function(nrounds, eta, max_depth,gamma) {
    
    xgb_model <- xgb.train(
        params = list(
            booster = "gbtree"
            ,objective = "reg:linear"
            ,eval_metric = "rmse"
            ,eta = eta
            ,max_depth=max_depth
            ,gamma = gamma)
    ,data = my_train
    ,watchlist = watchlist
    ,nrounds = nrounds
    ,early_stopping_rounds = 10
    ,verbose = TRUE
    ,prediction = TRUE) 
    
    list(
        #Score = xgb_model$best_score
        Score = -xgb_model$evaluation_log[xgb_model$best_iteration]$test_rmse
        ,Pred = xgb_model$pred)
  
}

set.seed(1)
opt_res <- BayesianOptimization(opt_fn
                                ,bounds = list(
                                    nrounds = c(50,500)
                                    ,eta = c(0.01,0.5)
                                    ,max_depth = c(2L,6L)
                                    ,gamma = c(0,20)
                               )
                                ,init_grid_dt = NULL
                                ,init_points = 10
                                ,n_iter = 30
                                ,acq = "ucb"
                                ,kappa = 2.576
                                ,eps = 0.0
                                ,verbose = TRUE)


 


```

### Bayesian Optimisation (PART II)
Steps :
1. Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate

The best we have got so far
nrounds = 400
eta = 0.2058	
max_depth = 5.0000	
gamma = 3.6674	
Test RMSE  = -0.9625 


2. We will fix the above value

3. Use bayesopt to tune
min_child_weight
subsample
colsample_bytree


---- We will go with nrounds 400, eta of 0.2. Best we have got so far ---------

2. Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. 

3. Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.

4. Lower the learning rate and decide the optimal parameters .



```{r}

opt_fn <- function(min_child_weight,subsample,colsample_bytree
) {
    
    xgb_model <- xgb.train(
        params = list(
            booster = "gbtree"
            ,objective = "reg:linear"
            ,eval_metric = "rmse"
            ,eta = 0.2
            ,max_depth=5
            ,gamma = 4
            ,min_child_weight = min_child_weight
            ,subsample = subsample
            ,colsample_bytree = colsample_bytree
            )
    ,data = my_train
    ,watchlist = watchlist
    ,nrounds = 200
    ,early_stopping_rounds = 10
    ,verbose = TRUE
    ,prediction = TRUE) 
    
    list(
        #Score = xgb_model$best_score
        Score = -xgb_model$evaluation_log[xgb_model$best_iteration]$test_rmse
        ,Pred = xgb_model$pred)
  
}

set.seed(1)
opt_res <- BayesianOptimization(opt_fn
                                ,bounds = list(
                                    min_child_weight = c(1L,3L)
                                    ,subsample = c(0.8,1)
                                    ,colsample_bytree = c(0.8,1)

                               )
                                ,init_grid_dt = NULL
                                ,init_points = 10
                                ,n_iter = 30
                                ,acq = "ucb"
                                ,kappa = 2.576
                                ,eps = 0.0
                                ,verbose = TRUE)





```



### Re-evaluate the best itin on my_test

eta = 0.2,max_depth=5,gamma = 4
Best iteration:  [238]	test-rmse:0.973781	train-rmse:0.948695

eta = 0.2,max_depth=4,gamma = 1,min_child_weight = 1,subsample = 0.9,colsample_bytree = 0.9


5th May 19. The best we have got so far. LB : 96.6886094264532
nrounds = 400
eta = 0.2058	
max_depth = 5.0000	
gamma = 3.6674	
Test RMSE  = -0.9625 

Without scaling
Stopping. Best iteration: [289]	test-rmse:0.972385	train-rmse:0.944414

with scaling, converges faster with better test RMSE (but are also random as seen in second try)
Stopping. Best iteration: [215]	test-rmse:0.971473	train-rmse:0.950535
Stopping. Best iteration: [219]	test-rmse:0.979283	train-rmse:0.948156

** Scaling will be used henceforth ** 

After accounting for the Tot_pax outliers (group by features)
Stopping. Best iteration: [198]	test-rmse:0.967705	train-rmse:0.948594
On full dataset gave :
Stopping. Best iteration: [181]	train-rmse:0.953192
On LB worsened to 97.3939561450232 !!!
Overfitting, may need to find different parameters

Added more member id - group by (~ interactions ?)
Not much improvement .. may need different parameters ?
Stopping. Best iteration: [274]	test-rmse:0.969651	train-rmse:0.938974
On full dataset gave :
Stopping. Best iteration: [236]	train-rmse:0.945570
On LB : Did not try


Random forest tend to prefer continuous variables
Source : https://stackoverflow.com/questions/51601122/xgboost-minimize-influence-of-continuous-linear-features-as-opposed-to-categori

* When LOS was factorised, it disappeared as top gain feature
[200]	test-rmse:0.978460	train-rmse:0.938315 

* LOS new is numeric
[200]	test-rmse:0.977013	train-rmse:0.938877 
Feature /Gain /Cover
no_of_rooms	0.16112941321	0.02965163869476	
length_of_stay_new	0.10876715108	0.01307624159602	
total_pax.2	0.06647513055	0.00899993784670	
roomnights	0.04114446519	0.02850132133291	
resort_id_checkin_date_month	0.03605975888	0.01641730914433	
persontravellingid_length_of_stay	0.02864570289	0.01208988920054	
resort_id_memberid	0.02838646881	0.01389018024741	
resort_id_length_of_stay	0.01775251701	0.01333075930600	
persontravellingid.47	0.01704931531	0.00496852309983	
checkin_date_year_memberid	0.01620617944	0.01399446282786	


```{r}

xgb_model <- xgb.train(
    params = list(
             booster = "gbtree"
            ,objective = "reg:linear"
            ,eval_metric = "rmse"
        ,eta = 0.2058
        ,max_depth=5
        ,gamma = 3.6674
        
        # ,min_child_weight = 1
        ,subsample = 0.8
        ,colsample_bytree = 0.8

        )
,data = my_train
,watchlist = watchlist
,nrounds = 200
,early_stopping_rounds = 3
,verbose = TRUE
,prediction = TRUE) 



# Learning Curve
ggplot(melt(xgb_model$evaluation_log,id.vars = "iter")) +
    geom_line(aes(x=iter, y=value, color=variable))

importance <- xgb.importance(model = xgb_model)
importance
xgb.plot.importance(importance)

importance[1:30]$Feature

# Other XGB plots [TO BE REVIEWED]
#xgb.plot.multi.trees(model = xgb_model)
#xgb.plot.deepness(model=xgb_model)
#xgb.plot.shap()
xgb_model$best_iteration
#xgb.plot.tree(model=xgb_model,trees = 1)


# Feature contributions [HOW CAN THIS BE USED]
#pred <- predict(xgb_model,newdata = my_test ,predcontrib = TRUE)



#Find linear combinations or correlated variables and remove from data
if(!require(caret,quietly = TRUE)) { install.packages("caret"); library(caret)}
mtcars$wt_new <- mtcars$wt * 2
#NAs gives error
lincomb <- findLinearCombos(mtcars[complete.cases(mtcars),])
lincomb$linearCombos[[1]]
colnames(mtcars)[lincomb$linearCombos[[1]]]
mtcars[-lincomb$remove]


```


### Final train on full train


#### Try 1
nrounds = 427.3809	eta = 0.3832	max_depth = 3.0000	gamma = 0.0000	Value = -0.9835 
Gave LB 96.7602365861488

#### Try 2
nrounds = 746.5130	eta = 0.2058	max_depth = 5.0000	gamma = 3.6674	Value = -0.9625 
Stopping. Best iteration:
[418]	test-rmse:0.951616	train-rmse:0.947857
Gave LB Score of 96.6886094264532

#### Try 3
nrounds = 300.0000	eta = 0.1889	max_depth = 4.0000	gamma = 4.8701	Value = -0.9729 
Stopping. Best iteration:
[193]	test-rmse:0.973284	train-rmse:0.963898
Gave LB Score of 97.02




```{r}


my_train <- DT_bkup[flag == "train",]

my_train_labels <- my_train$amount_spent_per_room_night_scaled
my_train$amount_spent_per_room_night_scaled <- NULL

dummies <- dummyVars(~., data = my_train, fullRank = TRUE)
my_train <- predict(dummies, newdata = my_train)

my_train <- xgb.DMatrix(my_train, label = my_train_labels)

xgb_model <- xgboost(
    params = list(
             booster = "gbtree"
            ,objective = "reg:linear"
            ,eval_metric = "rmse"
        #     
        # ,min_child_weight = 1
        # ,subsample = 0.9
        # ,colsample_bytree = 0.9

        ,eta = 0.2058
        ,max_depth=5
        ,gamma = 3.6674
)
,data = my_train
,nrounds = 275
,early_stopping_rounds = 10
,verbose = TRUE
,prediction = TRUE) 


```





### Submission
```{r}
my_test <- DT_bkup[flag == "test",]
my_test$amount_spent_per_room_night_scaled <- NULL

dummies <- dummyVars(~., data = my_test,fullRank = TRUE)
my_test <- predict(dummies, newdata = my_test)

my_test <- xgb.DMatrix(my_test)

pred <- predict(xgb_model,newdata = my_test)
summary(pred)

get_unique_ids <- fread(here::here("100_data_raw-input","test.csv"))

submit_kaggle <- as.data.frame(cbind(get_unique_ids$reservation_id,pred))
colnames(submit_kaggle) <- c("reservation_id"
                             ,"amount_spent_per_room_night_scaled")

write.csv(submit_kaggle
          ,file = here("120_data_output"
                       ,paste0("Submit_XGB_",strftime(Sys.time(), format="%Y%m%d_%H%M%S"),".csv"))
          ,row.names = FALSE)
here()

```








